{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a744ca1f",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0962572",
   "metadata": {},
   "source": [
    "Why is clustering useful?\n",
    "- for data analysis - when you analyze a new dataset it can be usfelu running a clustering algorithm and then analysing each cluster separately\n",
    "- dimensionality reduction - when you have k clusters you can replace an instance x with the vector of its cluster affinities. This will wiled a lower k-dimensional vector\n",
    "- Segmenting an image - if we cluster the data according to their color and then replace each instance color with the respective mean cluster color, it is possible to reduce the overall number of colors in an image, making it easier to analyze. Note that this is just another dimensionality reduction task.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440044d9",
   "metadata": {},
   "source": [
    "#### k-means\n",
    "This one here is the easiest yet powerful clustering algorithm. It is a parametric one, since it assumes the clusters in the data to have a spherical shape. \n",
    "\n",
    "**Note** Scaling feature beforehand generally improves the algorithm performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824aa44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "x,y = make_blobs(n_samples=500, random_state=42, centers=5)\n",
    "\n",
    "k = 5 # the algorithm will detect five clusters\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "y_pred = kmeans.fit_predict(X=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e9e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data visualization\n",
    "from matplotlib.pyplot import scatter\n",
    "scatter(x[:,0], x[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf3c6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f57dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall result\n",
    "import matplotlib.pyplot as plt\n",
    "def plotting_clusters(alg, x):\n",
    "    # data \n",
    "    plt.plot(x[:,0], x[:,1], \"y^\", label = \"data\")\n",
    "    # centroid coordinates\n",
    "    centers = alg.cluster_centers_\n",
    "    i = 0\n",
    "    for c in centers:\n",
    "        plt.plot(c[0], c[1], \"bs\", label = f\"centroid {i}\")\n",
    "        i += 1\n",
    "    plt.legend(loc=\"lower right\", fontsize=8)\n",
    "plotting_clusters(kmeans, x)\n",
    "# plt.savefig(\"k means cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad398c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction making\n",
    "X_new, y_new = make_blobs(20, random_state=42, centers= 5)\n",
    "kmeans.predict(X_new)\n",
    "plotting_clusters(kmeans, X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35384cf6",
   "metadata": {},
   "source": [
    "In general k-means is an algorithm used to perform hard-clustering, meaning that each instace is assignes only to one cluster per time. In some cases it might be useful to be a little more conservative, performing soft-sclutering. In this case the algorithm will output a score (often a probability) for each point to belong to a cluster. In k-means it is reasonable thinking that such a score coud be the distance of each point to the centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4197af",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.transform(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3839bed4",
   "metadata": {},
   "source": [
    "In general k-means algorithm places the centrois in a random way choosing k instances among the data at random. If you already have a sense about where the centroids should be placed at the beginning, you can tell their coordinates. \n",
    "\n",
    "In order to avoid doing this, in sklearn the centroid placing method used is the K-means++ by default. The goal is finding an efficient way of spreading the centroids the most. Here's how it works:\n",
    "1. select the first centroid at random within the training instances\n",
    "2. assign to each pint in the dataset a probility measure proportional to the distance squared from the closest centroid\n",
    "3. draw a new centroid accoringly to the probabilities assigned to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c363365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting manually the centroids\n",
    "start = ([[-3,3], [-3,2], [-3,1], [-1,2], [0,2]])\n",
    "kmeans_new = KMeans(n_clusters=5, init=start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5378d327",
   "metadata": {},
   "source": [
    "Finally we need to address the last two important things:\n",
    "- assessing model performance - this can be done by using as performance measure the sum over all datapoints of the squared distance from the respective centroid\n",
    "- controoling the number of runs performed - kmeans strongly depedns from the initial centroids position. Since we place them casually different runs of the algorithm will yield different results. By default sklearn runs the algorithm 10 times and outputs the best clustering results, based upon the above performance measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467cfe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance measure: inertia\n",
    "kmeans.inertia_\n",
    "# score: - inertia (every score in sklearn is interpreted by the rule \"greater is better\")\n",
    "kmeans.score(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5025e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the number of iterations to be performed\n",
    "kmeans_new = KMeans(n_clusters=4, n_init=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4fe286",
   "metadata": {},
   "source": [
    "##### Finding the optimal number of clusters\n",
    "\n",
    "The number of clusters k in k-means in the only hyperparameter to set. It impacts a lot the model performance, so it pretty important getting if right. The best idea is using some performance measures, like the inertia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e1ec1f",
   "metadata": {},
   "source": [
    "Using inertia we could make a plot of model inertia wrt to the number of clusters. Then we look for the elbow in the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780ff9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dgp\n",
    "x,y = make_blobs(n_samples=500, random_state=42, centers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6636364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model creation and training\n",
    "kmeans = KMeans(n_clusters=5, random_state= 42)\n",
    "kmeans.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b18378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import arange\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "k_val = arange(1,10)\n",
    "performance = []\n",
    "\n",
    "for k in k_val: \n",
    "    k = int(k)\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(x)\n",
    "    performance.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(k_val, performance,\"o-r\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"inertia\")\n",
    "# plt.savefig(\"elbow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de465ba4",
   "metadata": {},
   "source": [
    "Another important performance measure for clustering is the silhoutte coefficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687e72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from numpy import arange\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "k_val = arange(2,10)\n",
    "performance = []\n",
    "\n",
    "for k in k_val: \n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(x)\n",
    "    labels = kmeans.labels_\n",
    "    performance.append(silhouette_score(x, labels))\n",
    "\n",
    "plt.plot(k_val, performance,\"o-b\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"silhoutte score\")\n",
    "plt.savefig(\"silhoutte_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b28b2d",
   "metadata": {},
   "source": [
    "The silhoutte coefficient give more information then the inertia score. So better be using it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333c6c96",
   "metadata": {},
   "source": [
    "##### k-means application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc13abe",
   "metadata": {},
   "source": [
    "- preprocessing the data (dimensionality reduction) \n",
    "\n",
    "We cluster the data in k different clusters, then we substitute an instance features with cluster similarity for each cluaster we have found. This wai we'll reduce the training instances to be k-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17490986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_digits # 8x8 pixel instances, 64 features\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0850ec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f3b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902fe825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before using clustering for preprocessing the data\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d5fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using clustering for preprocessing the data\n",
    "model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"kmeans\", KMeans(n_clusters=50)), # instances now have 50 features, one for each culster similarity score \n",
    "    (\"log_reg\", LogisticRegression(max_iter=3000))\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dbc59f",
   "metadata": {},
   "source": [
    "Another important feature of using clustering for data preprocesing is that the number of clusters k is just another hyperparameter of the overall model. So we can find the best value for it by GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4a9708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = dict(kmeans__n_clusters=range(2,100))\n",
    "grid_clf = GridSearchCV(model, param_grid=param_grid, cv =3)\n",
    "grid_clf.fit(X_train, y_train)\n",
    "grid_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523d556f",
   "metadata": {},
   "source": [
    "#### DBSCAN and HBDSCAN\n",
    "\n",
    "This clustering algorithm can be seen as the non-parametric counterpart of k-means. It doesn't make any assumptions beforehand about the clusters form. Yet it is more difficult to tune, the value of $\\epsilon$ is very nasty to get right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaab89a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from matplotlib.pyplot import scatter\n",
    "\n",
    "X, y = make_moons(n_samples=1000, noise = 0.05)\n",
    "scatter(X[:,0], X[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c26df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps = 0.05, min_samples=5)\n",
    "dbscan.fit(X)\n",
    "dbscan.labels_  # note that this algorithm finds 7 different clusters, not a good job\n",
    "# label = -1 when the point has been labelles as noise and thus dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d3572b",
   "metadata": {},
   "source": [
    "If we want to avoid the burden of chosing $\\epsilon$ we may try ti use the HDSCAN algorithm. That is a non-parametric, hierarchical clustering algorithm (see notes for more). It has only one hyperparameter, that is the minimum number of instances that must be found in a cluster to be one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b611523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "hdbscan = HDBSCAN(min_cluster_size=5) # default: n_samples = 5\n",
    "hdbscan.fit(X)\n",
    "hdbscan.labels_ # 2 clusters are correctly found, labels = {0,1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a62904",
   "metadata": {},
   "source": [
    "This last algorithm is very powerfu and easy to use. A reccomandation is using it as work-horse. It is very flexible and pretti efficient, so you can alway use this one without worrying too much at a first approach with the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e98c99",
   "metadata": {},
   "source": [
    "#### Gaussian Mixtures\n",
    "\n",
    "Yet another parametric clustering algorithm. The difference here is that the clustering is performed following a soft approach, meaning that we ooutput a probability of each instance to belong to a certain cluster. \n",
    "\n",
    "The parametric assumption we make is that the data were drawm from a sum of multivariate gaussian distributions. The algorithm, loogking at the data, tries finding the parameters of each one of this mvn distributions. \n",
    "\n",
    "The number of mvn to consider is the only hyperparameter of this algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59e0590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dgp\n",
    "from sklearn.datasets import make_moons\n",
    "from matplotlib.pyplot import scatter\n",
    "\n",
    "X, y = make_moons(n_samples=1000, noise = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc0a721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gm = GaussianMixture(n_components=2, n_init = 10)\n",
    "gm.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters estimated\n",
    "print(f\"weights: {gm.weights_},means: {gm.means_},covarancies: {gm.covariances_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5f2b19",
   "metadata": {},
   "source": [
    "The parameter weights is an estimate of how important every mvn estimated is, meaning how many samples were most likely drawn from it: the higher, the more sampes were drawm from that distrbution (likely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40106be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking convergence \n",
    "gm.converged_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e363c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking number of iterations needed for converging\n",
    "gm.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d767392f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions: instances are assigned to the most likely cluster, highest probability computed\n",
    "gm.predict(X)\n",
    "# soft predictions: probs for every instance of being drawn from one of the mvn\n",
    "gm.predict_proba(X) \n",
    "# local probability density estimation \n",
    "# for each point given, this method uses the computed mvn distribution to output the approximate log pdf \n",
    "# value in that point\n",
    "gm.score_samples(X)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b34b0f",
   "metadata": {},
   "source": [
    "We can also limit the number of shapes the clusters we find. This will help the algorithm, decreasing the total number of parameters it needs to find in order to converge.\n",
    "\n",
    "The possible shapes you can set is:\n",
    "- \"spherical\": for spherical clusters\n",
    "- \"diag\": for ellipsoid clusters\n",
    "- \"tied\": for ellipsoid cluster of the same size\n",
    "\n",
    "By default the algorithm will look for clusters of any shape and size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a7897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = GaussianMixture(n_components=2, covariance_type=\"spherical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19bcdee",
   "metadata": {},
   "source": [
    "For assessing the performance of a gm model we can use penalized theoretical information criterion:\n",
    "- BIC (favoring less complex models)\n",
    "- AIC\n",
    "\n",
    "The higher they are, the worse the model is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c571ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm.bic(X)\n",
    "gm.aic(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f86f7f",
   "metadata": {},
   "source": [
    "Thus, they can be used for finding the best overall GMM, looking at what combination of hyperparameters yields the lowest value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a47df90",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0453ab",
   "metadata": {},
   "source": [
    "1. How would you define clustering? Can you name a few clustering algorithms?\n",
    "\n",
    "Clustering is an un-supervised learning task. It stands for the process of grouping the data we have wrt a similarity measure of our choice. \n",
    "\n",
    "Clustering algorithms can be divided according to two different criteria: parametric and non parametric, soft and hard.  \n",
    "\n",
    "One of them for each class: \n",
    "- parametric and hard: k-means \n",
    "- parametric and soft: Gaussian mixture models (GMM)\n",
    "- non-parametric and hard: DBSCAN\n",
    "- non-parametric and soft: HDBSCAN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133c7bff",
   "metadata": {},
   "source": [
    "2. Describe two techniques to select the right number of clusters when using k-means\n",
    "\n",
    "The first one is looking at the inertia score. The inertia value is computed pretty much a RSS, using the euclidean distance of a point from the nearest centroid for all the points in the dataset. The higher the inertia value, the worst the clustering result is. It also gets lower the more cluster we look for (as k increases).\n",
    "\n",
    "The second one is the silhoutte coefficient. This is computed for every point as:\n",
    "$$s_{\\text{coefficient}} = \\frac{b-a}{\\max{(b;a)}}$$\n",
    "where:\n",
    "- a is the mean distance of the point from the nearest centroid\n",
    "- b is the mean distance of the point from the other centroids, excluded the closest one. \n",
    "The silhoutte score of the algorithm is the mean value over the data points silhoutte score. \n",
    "\n",
    "It varies from [-1,1], the higher the better. It is 1 when $a=0$, this happens only if every point of each cluster is positioned exactly on top of the relative centroid: perfect clustering. It is -1 if $b=0$, meaning that every point is hardly bad-clustered in a cluster $C_j$, but finds itself on top of another centroid $C_k$ ( $j \\neq k$ )."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70cdea9",
   "metadata": {},
   "source": [
    "3. Can you name two algorithms that can scale on large datasets? can you name two algorithms that look for regions of high density? \n",
    "\n",
    "Two alg. that can scale on large datasets are: \n",
    "- mini-batch k-means\n",
    "- stochastic k-means\n",
    "\n",
    "Two alg. that look for regions of high density are:\n",
    "- DBSCAN (and the soft cousin HDBSCAN)\n",
    "- GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cfbfbc",
   "metadata": {},
   "source": [
    "4. What is a Gaussian mixture?\n",
    "\n",
    "A Gaussian mixture model (GMM) is a soft, parametric algorithm. The idea behing it is that the data we have, were drowm from a population having a probability distribution equal to the sum of various multivariate normal distributions. The algorithms, looking at how the data are spread across the feature space and using an algorithm on likelihood maximizing, find the parameters for all the mvn in the overall population we think mught exist. \n",
    "\n",
    "Note that this number is an hyperparameter of the model. Once the mvn are estimated, at each point a probability of belonging to a certaing cluster is assigned. Then a point is assigned to the cluster who's most probable being with, based upon the probabilities assegnid before. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee912107",
   "metadata": {},
   "source": [
    "5. Can you name two techniques to find the right number of clusters when using a GMM? \n",
    "\n",
    "For assessing the performance of a GMM, since for how it works it outputs an estimate of the overall population distribution, parametrized as a linear combination of mvn, penalized likelihood values for the model would be on point. For assessing the performance for this kind of models, we can use something like AIG and BIC, the lower they are, the better the model is. \n",
    "\n",
    "$$\\text{BIC} = p\\log(m) - 2 \\log(L)$$\n",
    "$$\\text{AIC} = 2p - 2 \\log(L)$$\n",
    "\n",
    "where:\n",
    "- m is the number of instances\n",
    "- p is the number of parameters the model learnt\n",
    "- L is the maximized likelihood function of the model. In general the likelihood gives the probability of a specific parameter value given the instances. The parameter value that maximizes the likelihood is called maximum likelihood estimator. \n",
    "\n",
    "A lower value of AIC and BIC means that the model has a great value for the maximized likelihood function, that is to say the parameters values estimated (the mean and covariance matrix of every mvn in the parametrized population distribution) are very likely to be the true ones, given the data we are working with. \n",
    "\n",
    "In a few words, if AIC and BIC are low we have done a great job. Thus they can be used as coefficients to select the right number of clusters when using GMM, which will be the one to minimize AIC and BIC. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92e5bbd",
   "metadata": {},
   "source": [
    "6. Perform clustering on the Olivetti faces dataset and fin-tune it, finding the right number of clusters. Then visualize the clusters: do you see similar faces in each cluster? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# 400 images of 64x64 pixels\n",
    "X,y = fetch_olivetti_faces(return_X_y=True)\n",
    "\n",
    "# performing stratified sampling to be sure that both in training and test set are representative of \n",
    "# all the data\n",
    "\n",
    "# creaing test and training set\n",
    "strat_split = StratifiedShuffleSplit(n_splits=1, test_size=40, random_state=42)\n",
    "# next() is used to return both the output values of the strat_split.split() method in one line\n",
    "train_valid_idx, test_idx = next(strat_split.split(X,y))\n",
    "X_train_val, y_train_val = X[train_valid_idx], y[train_valid_idx]\n",
    "X_test, y_test = X[test_idx], y[test_idx]\n",
    "\n",
    "# creating a validation set from the training set\n",
    "strat_split = StratifiedShuffleSplit(n_splits=1, test_size=80, random_state=43)\n",
    "train_idx, val_idx = next(strat_split.split(X_train_val, y_train_val))\n",
    "X_train, y_train = X_train_val[train_idx], y_train_val[train_idx]\n",
    "X_val, y_val = X_train_val[val_idx], y_train_val[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b30c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa1eddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, HDBSCAN, DBSCAN\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0e4c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing PCA for dimensionality reduction purposes\n",
    "pca = PCA(0.99)\n",
    "\n",
    "# perform PCA on the training set, since it has more instances \n",
    "pca.fit(X_train)\n",
    "\n",
    "# transform the data, projecting each instance in the lower dimensional pc space\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_val_pca = pca.transform(X_val)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b5952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training k-means models, k varying\n",
    "k_range = range(5, 150, 5)\n",
    "kmeans_per_k = []\n",
    "for k in k_range:\n",
    "    print(\"k={}\".format(k))\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(X_train_pca)\n",
    "    kmeans_per_k.append(kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ff1f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using inertia value to find the best k\n",
    "score = []\n",
    "\n",
    "for m in kmeans_per_k:\n",
    "    score.append(m.inertia_)\n",
    "\n",
    "plt.plot(k_range, score, \"o-r\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"inertia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d686fb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using silhoutte score to find the best k\n",
    "from numpy import argmax\n",
    "from sklearn.metrics import silhouette_score\n",
    "score = []\n",
    "\n",
    "for m in kmeans_per_k:\n",
    "    score.append(silhouette_score(X_train_pca, m.labels_))\n",
    "\n",
    "best_k_idx = argmax(score)\n",
    "best_k = k_range[best_k_idx]\n",
    "best_score = score[best_k_idx]\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(k_range, score, \"o-r\")\n",
    "plt.plot(best_k, best_score, \"bs\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"silhoutte coefficient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f337396",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_k, best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa75093",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = KMeans(n_clusters=best_k)\n",
    "best_model.fit(X_train_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab08929",
   "metadata": {},
   "source": [
    "It looks like the best number of clusters is quite high, at 75. You might have expected it to be 40, since there are 40 different people on the pictures. However, the same person may look quite different on different pictures (e.g., with or without glasses, or simply shifted left or right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa03754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the clusters\n",
    "import numpy as np\n",
    "\n",
    "def plot_faces(faces, labels, n_cols=5):\n",
    "    faces = faces.reshape(-1, 64, 64)\n",
    "    n_rows = (len(faces) - 1) // n_cols + 1\n",
    "    plt.figure(figsize=(n_cols, n_rows * 1.1))\n",
    "    for index, (face, label) in enumerate(zip(faces, labels)):\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.imshow(face, cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(label)\n",
    "    plt.show()\n",
    "\n",
    "for cluster_id in np.unique(best_model.labels_):\n",
    "    print(\"Cluster\", cluster_id)\n",
    "    in_cluster = best_model.labels_==cluster_id\n",
    "    faces = X_train[in_cluster]\n",
    "    labels = y_train[in_cluster]\n",
    "    plot_faces(faces, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5ecf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's try a HDBSCAN model \n",
    "hdb = HDBSCAN()\n",
    "hdb.fit(X_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14415961",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(X_train_pca, hdb.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b29b9c",
   "metadata": {},
   "source": [
    "An HDBSCAN performs badly on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5174d2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try DBSCAN, fine tuning the epsilon parameter\n",
    "\n",
    "eps_range = np.arange(100,105, 0.1,)\n",
    "models = []\n",
    "\n",
    "for eps in eps_range:\n",
    "    model = DBSCAN(eps=eps)\n",
    "    print(\"training DBSCAN with epsilon={}\".format(eps))\n",
    "    models.append(model.fit(X_train_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c49b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for model in models:\n",
    "    print(model.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0aaa2a",
   "metadata": {},
   "source": [
    "The DBSCAN algorithm doesn't work. it only fids one big cluster, even for very high values of epsilon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09815cda",
   "metadata": {},
   "source": [
    "7. Continuing on the Olivetti faces dataset, use k-means as a dimensionality reduction tool and check if the overall model performance improves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3213013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train_pca, y_train)\n",
    "rfc.score(X_val_pca,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae252b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using k means as a dimensionality reduction algorithm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "pip = Pipeline([\n",
    "    (\"kmeans\", KMeans()),\n",
    "    (\"rf\", RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9653aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tuning the hyperparameter \n",
    "k_range = range(5, 150, 5)\n",
    "scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    pip = pip = Pipeline([\n",
    "    (\"kmeans\", KMeans(n_clusters=k)), # transform the feature space in a cluster distance space\n",
    "    (\"rf\", RandomForestClassifier())\n",
    "])\n",
    "    pip.fit(X_train_pca, y_train)\n",
    "    print(f\"k={k}, accuracy={pip.score(X_val_pca, y_val)}\")\n",
    "    scores.append(pip.score(X_val_pca, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494260ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k_idx = argmax(scores)\n",
    "best_k = k_range[best_k_idx]\n",
    "best_score = scores[best_k_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24286e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k_range, scores, \"o-b\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.plot(best_k, best_score, \"rs\")\n",
    "plt.figure(figsize=(10,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d08d085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best k\n",
    "print(best_k, best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7bf8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison \n",
    "print(rfc.score(X_val_pca, y_val))\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892ae840",
   "metadata": {},
   "source": [
    "rfc performed clearly better without using the kmeans as a dimensionality reduction algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5b71cd",
   "metadata": {},
   "source": [
    "We could also try to add the cluster distance features to each instance in tho orgiinal data set and check if the proformance of the model gets better, having data with more features to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f281a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "for n_clusters in k_range:\n",
    "    pipeline = Pipeline([\n",
    "        (\"kmeans\", KMeans(n_clusters=n_clusters, random_state=42)),\n",
    "        (\"forest_clf\", RandomForestClassifier(n_estimators=150, random_state=42))\n",
    "    ])\n",
    "    pipeline.fit(X_train_pca, y_train)\n",
    "    print(n_clusters, pipeline.score(X_val_pca, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dd51da",
   "metadata": {},
   "source": [
    "With k=140 we obtained a score of 0.8, a better performance overall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd04829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble learning: hard voting classifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "vc = VotingClassifier([\n",
    "    (\"gb\", GradientBoostingClassifier()),\n",
    "    (\"svc\", LinearSVC()),\n",
    "    (\"log_reg\", LogisticRegression())\n",
    "], voting=\"hard\")\n",
    "\n",
    "vc.fit(X_train_pca, y=y_train)\n",
    "\n",
    "# training time: 2m 20s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e391bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pred = vc.predict(X_val_pca)\n",
    "score = accuracy_score(y_true=y_val, y_pred=pred)\n",
    "print(score)\n",
    "\n",
    "# acuracy: 0.975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4d7bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using k-means as a dimensionality reduction algorithm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pip = Pipeline([\n",
    "    (\"kmeans\", KMeans(random_state=42)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"ensemble model\", vc)\n",
    "])\n",
    "\n",
    "pip.fit(X_train_pca, y_train)\n",
    "print(accuracy_score(y_pred = pip.predict(X_val_pca), y_true=y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a656df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k fine-tuning, using a standard scaling beforehand\n",
    "# gridshearch would be too much for tuning just one hyperparameter\n",
    "\n",
    "K_range = range (10,150,10)\n",
    "scores = []\n",
    "\n",
    "for k in K_range:\n",
    "    pip = Pipeline([\n",
    "    (\"kmeans\", KMeans(n_clusters=k)),  \n",
    "    (\"scaler\", StandardScaler()), # scaling the transformed feature vector, for making the training faster and better\n",
    "    (\"ensemble model\", vc)\n",
    "    ])\n",
    "    pip.fit(X_train_pca,y_train)\n",
    "    print(\"k={}\".format(k))\n",
    "    scores.append(accuracy_score(y_pred=pip.predict(X_val_pca), y_true=y_val))\n",
    "    print(\"score={}\".format(accuracy_score(y_pred=pip.predict(X_val_pca), y_true=y_val)))\n",
    "\n",
    "# finding the maximum\n",
    "best_k_idx = np.argmax(scores)\n",
    "best_k = K_range[best_k_idx]\n",
    "best_score = scores[best_k_idx]\n",
    "\n",
    "print(best_k, best_score)\n",
    "\n",
    "# WARNING: around 10 minutes are needed to run this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e170329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(K_range, scores, \"o-b\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"accuracy score\")\n",
    "plt.plot(best_k, best_score, \"sr\", label = f\"k = {best_k}, accuracy = {best_score}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a2672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final ensemble model using Kmeans\n",
    "pip = Pipeline([\n",
    "    (\"kmeans\", KMeans(n_clusters=120, random_state=42)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"ensemble model\", vc)\n",
    "])\n",
    "pip.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7b0e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(pip.predict(X_val_pca), y_val))\n",
    "# score = 0.9625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee55ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "some_face = X_val[67] # changing the index here shows a different number\n",
    "some_face_image = some_face.reshape(64,64)\n",
    "\n",
    "plt.imshow(some_face_image, cmap = \"viridis\")\n",
    "plt.title(f\"face: {y_val[67]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16eefc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the best model for predictions\n",
    "vc.fit(X_train_pca, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2924e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc.predict(X_val_pca)[67]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b2515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(vc.predict(X_val_pca), y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
