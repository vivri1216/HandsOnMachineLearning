{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d84c20bf",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa351a67",
   "metadata": {},
   "source": [
    "#### Regular PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1bf6fc",
   "metadata": {},
   "source": [
    "A cool feature of sklern PCA object is that it handles automoatically the centering of the data, so that everything can work thine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cac8316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# tiy training set\n",
    "X = np.array([[0, 1] , [2,3]])\n",
    "\n",
    "# using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X= X) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8b71e8",
   "metadata": {},
   "source": [
    "If standardizig the data is necessary for solving feature scale ussues, then the following code does the work.It will set the mean and variance of every feature to be, respectively, zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda63938",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "workflow = Pipeline([\n",
    "    (\"scaler\", StandardScaler),\n",
    "    (\"pca\", pca),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89768cca",
   "metadata": {},
   "source": [
    "Anothe cool thing of sklearn PCA is that instead of telling how many principal components you want, you can tell it how much of the data total variance you want to preserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318db473",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95) # 95% variance preserved\n",
    "X_reuced = pca.fit_transform(X)\n",
    "\n",
    "pca.explained_variance_ratio_ # this one returs a list with the percentage of variance explained by the respective feature "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5995d3b",
   "metadata": {},
   "source": [
    "For creating a plot showing the % of variance exaplained wrt the number of principal components considered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c740b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.plot(cumsum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12610e53",
   "metadata": {},
   "source": [
    "#### Randomized PCA \n",
    "This one uses a stochastic optimiation algorithm, leveraging the SVD. It is less computationally expensive than classic PCA and thus it has to be used when the dataset starts getting very big. infact standard PCA is pretty slow and expensive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95a21ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components = 154, svd_solver=\"randomized\")\n",
    "X_reduced = rnd_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6be592",
   "metadata": {},
   "source": [
    "Note that this solver is automatically used when the data become big, by conditions set into the PCA() class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959d3c53",
   "metadata": {},
   "source": [
    "#### Kernal PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85513ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# creating and usng it\n",
    "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.04)\n",
    "X = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c1acf0",
   "metadata": {},
   "source": [
    "How can we choose the right kernel? Realistically we can treat the kernel as a hyperparameter of the model, thus finding the best one through GridSearch and cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900bef89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# build a pipeline, the logistic regression will take as input the kpca ouput to make predictions \n",
    "clf = Pipeline([\n",
    "    (\"kpca\", KernelPCA(n_components=2)),\n",
    "    (\"log_reg\", LogisticRegression())\n",
    "])\n",
    "\n",
    "# set a grid search\n",
    "param_grid = [{\n",
    "    \"kpca__gamma\": np.linspace(0.03,0.05,10),\n",
    "    \"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n",
    "}]\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid, cv = 2)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# look at the best set of parameters found between the tested ones\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a956bd5",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8651ba01",
   "metadata": {},
   "source": [
    "1. What are the main motivations for reducing a dataset dimensionality? What are the main drawbacks?\n",
    "\n",
    "PRO: \n",
    "- having less dimensions to work with reduces the time needed for the model training\n",
    "- if the dimensions are reduced to 2 or 3 then the data can be visualized\n",
    "- removing some dimensions can, occasionally, remove redundancies in the data, improving the overall model\n",
    "\n",
    "CONS:\n",
    "- information loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23543ad4",
   "metadata": {},
   "source": [
    "2. What is the curse of dimensionality?\n",
    "\n",
    "The curse of dimensionality is a way to address the difficulties that come from having to deal with high-dimensional data. Specifically in higher dimensions optimizing becomes more difficult and also the data we have may not reppresent in the best way possible the overall population, causing our model to perform way worse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09118461",
   "metadata": {},
   "source": [
    "3. Can PCA be used to reduce the dimensionality f a highly non-linear dataset?\n",
    "\n",
    "No, it cannot. Infact PCA assumes that our data can be moslty explained on hyperplanes, linear subspaces in the feature space. If this assumption doesn't hold, then PCA won't work. For this reason we have also talked of Kernal PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e497adc8",
   "metadata": {},
   "source": [
    "4. Does it make any sense to chain two different dimensionality reduction algorithm on your data set? \n",
    "\n",
    "No, it does not. Infact one is sufficient to reduce the dimensionality of the data. Any oter one would just increase the overall information loss we accept by performing dimensionality reduction on the data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
